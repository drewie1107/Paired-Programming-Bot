{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Creating a code companion bot Using OpenAI API."
      ],
      "metadata": {
        "id": "ZMhpFiIw6HuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installing the necessary packages"
      ],
      "metadata": {
        "id": "vREtjwoJ6RAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "smFML1JtPYFf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "2zxkbwlzQvWp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installing the necessary libraries and setting up API"
      ],
      "metadata": {
        "id": "n772cuOj6W0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "import json\n",
        "import re\n",
        "\n",
        "openai.api_key = 'Your_API_Key'"
      ],
      "metadata": {
        "id": "6FvHnX3qQgHm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting up the chat function\n",
        "\n",
        "Here I am creating a chat function that takes in system prompt (initialization context), user prompt (input), temperature (creativity of the model's response), and the OpenAI model we are going to use"
      ],
      "metadata": {
        "id": "zeSxkZQ-6cp0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(system_prompt, user_prompt, model = 'gpt-3.5-turbo', temperature = 0, verbose = False):\n",
        "    ''' Normal call of OpenAI API '''\n",
        "    response = openai.ChatCompletion.create(\n",
        "    temperature = temperature,\n",
        "    model=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ])\n",
        "\n",
        "    res = response['choices'][0]['message']['content']\n",
        "\n",
        "    if verbose:\n",
        "        print('System prompt:', system_prompt)\n",
        "        print('User prompt:', user_prompt)\n",
        "        print('GPT response:', res)\n",
        "\n",
        "    return res"
      ],
      "metadata": {
        "id": "YpekOmBSQmNc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Testing if the model works"
      ],
      "metadata": {
        "id": "z9VKKyHB7QJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat(system_prompt='You are to classify if a number is divisible by 17 or not', user_prompt = '170')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3Mtc-epBQnrR",
        "outputId": "03845408-c982-4f9a-c944-8ba0fa32dbbc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Yes, 170 is divisible by 17.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import io\n",
        "import IPython.display\n",
        "from PIL import Image\n",
        "import base64\n",
        "import requests, json\n",
        "import gradio as gr\n",
        "requests.adapters.DEFAULT_TIMEOUT = 60"
      ],
      "metadata": {
        "id": "aVuxzOnFQrqs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optional - Giving  context for the bot to refer to"
      ],
      "metadata": {
        "id": "4f-WJ8wk8Aeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  context = '''Background\n",
        "#  Python programm\n",
        "#  '''"
      ],
      "metadata": {
        "id": "c_Zlh_YfQuCZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Defining necessary chatbot functions"
      ],
      "metadata": {
        "id": "TovHKF-e8oeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_prompt(message, chat_history, max_convo_length):\n",
        "    prompt = \"\"\n",
        "    for turn in chat_history[-max_convo_length:]:\n",
        "        user_message, bot_message = turn\n",
        "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
        "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "def respond(message, chat_history, max_convo_length = 10):\n",
        "        formatted_prompt = format_chat_prompt(message, chat_history, max_convo_length)\n",
        "        print(formatted_prompt)\n",
        "        bot_message = chat(system_prompt = f'''You are a junior developer bot who is still learning. You are paired with the user for paired programming assignment.\n",
        "You will always generate syntax wrong code, but you will correct the code once the user tells you to.''',  # for context, change to system_prompt = f'''text. Context: {context}'''\n",
        "                           user_prompt = formatted_prompt)\n",
        "        chat_history.append((message, bot_message))\n",
        "        return \"\", chat_history\n"
      ],
      "metadata": {
        "id": "gZ_P4qqCQ9L3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Launching the chatbot"
      ],
      "metadata": {
        "id": "DkLuEw0H80kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=300) #just to fit the notebook\n",
        "    msg = gr.Textbox(label=\"Prompt\")\n",
        "    with gr.Accordion(label=\"Advanced options\",open=True):\n",
        "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
        "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    btn.click(respond, inputs=[msg, chatbot], outputs=[msg, chatbot])\n",
        "    msg.submit(respond, inputs=[msg, chatbot], outputs=[msg, chatbot]) #Press enter to submit\n",
        "gr.close_all()\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "ZUETdcrV85aH",
        "outputId": "9546e324-d836-4528-b5d3-7fe607f3735f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:902: UserWarning: api_name respond already exists, using respond_1\n",
            "  warnings.warn(f\"api_name {api_name} already exists, using {api_name_}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://9866cc1ab433e326aa.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9866cc1ab433e326aa.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Referenced from https://learn.deeplearning.ai/huggingface-gradio/lesson/6/chat-with-any-llm\n",
        "* Referenced from https://platform.openai.com/docs/guides/gpt/chat-completions-api"
      ],
      "metadata": {
        "id": "UQK8SlmT9L5M"
      }
    }
  ]
}
