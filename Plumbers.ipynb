{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Installing necessary packages"
      ],
      "metadata": {
        "id": "TtlQ4ka-_cBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install utils"
      ],
      "metadata": {
        "id": "TF0sIQ5G3whd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google.generativeai"
      ],
      "metadata": {
        "id": "bwUP8fMZ30Ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google.api_core"
      ],
      "metadata": {
        "id": "mJjPi8gU4frM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "FRah1IjT7MRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing necessary libraries"
      ],
      "metadata": {
        "id": "z2H8uaDp_hbS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TiOHqTF63gIE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import IPython.display\n",
        "from PIL import Image\n",
        "import base64\n",
        "import requests\n",
        "requests.adapters.DEFAULT_TIMEOUT = 60"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as palm\n",
        "from google.api_core import client_options as client_options_lib\n"
      ],
      "metadata": {
        "id": "3KYLmw7-3j0l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up PaLM API"
      ],
      "metadata": {
        "id": "kUFc9OeK_v3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "palm.configure(\n",
        "    api_key= 'Your_API_Key',\n",
        "    transport=\"rest\",\n",
        "    client_options=client_options_lib.ClientOptions(\n",
        "        api_endpoint=os.getenv(\"GOOGLE_API_BASE\"),\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "c3GiZQXn4YQg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting up model"
      ],
      "metadata": {
        "id": "2IPZX0Al_2YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
        "model_bison = models[0]\n",
        "model_bison"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcFcLuMt4srd",
        "outputId": "f27e23aa-63f0-4aa9-911a-320a8e77a105"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(name='models/text-bison-001',\n",
              "      base_model_id='',\n",
              "      version='001',\n",
              "      display_name='Text Bison',\n",
              "      description='Model targeted for text generation.',\n",
              "      input_token_limit=8196,\n",
              "      output_token_limit=1024,\n",
              "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
              "      temperature=0.7,\n",
              "      top_p=0.95,\n",
              "      top_k=40)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Setting up text generation function"
      ],
      "metadata": {
        "id": "Rd0Tn68Z_63b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "@retry.Retry()\n",
        "def generate_text(prompt,\n",
        "                  model=model_bison,\n",
        "                  temperature=0.0):\n",
        "    return palm.generate_text(prompt=prompt,\n",
        "                              model=model,\n",
        "                              temperature=temperature)"
      ],
      "metadata": {
        "id": "1hGFOo6o5HAS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing promp tempelate to set up context"
      ],
      "metadata": {
        "id": "wzYWhDCxADTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"\n",
        "You are a junior developer bot who is still learning. You are paired with the user for paired programming assignment.\n",
        "You will always generate syntax wrong code, but you will correct the code once the user tells you to.\n",
        "\n",
        "Here is the user prompt\n",
        "\n",
        "{question}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "T5SiEQhX7CDp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launching Chatbot"
      ],
      "metadata": {
        "id": "MjCMyQIbAI1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import os"
      ],
      "metadata": {
        "id": "0dPuYQd16SXY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(input, slider):\n",
        "  \"\"\"Generates text based on the given prompt and max new tokens.\"\"\"\n",
        "  #prompt=prompt_template.format(question=input)\n",
        "  response = generate_text(prompt=prompt_template.format(question=input))\n",
        "  outputs = response.result\n",
        "  return outputs\n",
        "\n",
        "demo = gr.Interface(\n",
        "                    fn=generate,\n",
        "                    inputs=[gr.Textbox(label=\"Prompt\"),\n",
        "                            gr.Slider(label=\"Max new tokens\", value=20, maximum=2048, minimum=1)],\n",
        "                    outputs=[gr.Textbox(label=\"Output\")],\n",
        "                    title=\"Python Code Explainer\"\n",
        "                    )\n",
        "gr.close_all()\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "_c7J2IoeI4hM",
        "outputId": "e19073cf-762d-47e1-ca23-8c16d054d55c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://35b933a47c5d1d036a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://35b933a47c5d1d036a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up chatbot with conversational memory"
      ],
      "metadata": {
        "id": "Q0Nw_a0uBDtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_prompt(message, chat_history, instruction):\n",
        "    prompt = f\"System:{instruction}\"\n",
        "    for turn in chat_history:\n",
        "        user_message, bot_message = turn\n",
        "        prompt = f\"{prompt}\\nUser: {user_message}\\nAssistant: {bot_message}\"\n",
        "    prompt = f\"{prompt}\\nUser: {message}\\nAssistant:\"\n",
        "    return prompt\n",
        "\n",
        "def respond(message, chat_history, instruction, temperature=0.7):\n",
        "    prompt = format_chat_prompt(message, chat_history, instruction)\n",
        "    chat_history = chat_history + [[message, \"\"]]\n",
        "    stream = generate_text(prompt,\n",
        "                                      stop_sequences=[\"\\nUser:\", \"<|endoftext|>\"],\n",
        "                                      temperature=temperature)\n",
        "                                      #stop_sequences to not generate the user answer\n",
        "    acc_text = \"\"\n",
        "    #Streaming the tokens\n",
        "    for idx, response in enumerate(stream):\n",
        "            text_token = response.token.text\n",
        "\n",
        "            if response.details:\n",
        "                return\n",
        "\n",
        "            if idx == 0 and text_token.startswith(\" \"):\n",
        "                text_token = text_token[1:]\n",
        "\n",
        "            acc_text += text_token\n",
        "            last_turn = list(chat_history.pop(-1))\n",
        "            last_turn[-1] += acc_text\n",
        "            chat_history = chat_history + [last_turn]\n",
        "            yield \"\", chat_history\n",
        "            acc_text = \"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=240) #just to fit the notebook\n",
        "    msg = gr.Textbox(label=\"Prompt\")\n",
        "    with gr.Accordion(label=\"Advanced options\",open=False):\n",
        "        system = gr.Textbox(label=\"System message\", lines=2, value=\"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.\")\n",
        "        temperature = gr.Slider(label=\"temperature\", minimum=0.1, maximum=1, value=0.7, step=0.1)\n",
        "    btn = gr.Button(\"Submit\")\n",
        "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
        "\n",
        "    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])\n",
        "    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit\n",
        "gr.close_all()\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "5OrTegtnkMfr",
        "outputId": "db3ae78c-b16f-4621-ef4d-0a18b3ff5c50"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:902: UserWarning: api_name respond already exists, using respond_1\n",
            "  warnings.warn(f\"api_name {api_name} already exists, using {api_name_}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closing server running on port: 7860\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://ddd887c712cdc6edcd.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ddd887c712cdc6edcd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you can see that the model does not output response because PaLM api models do not have a method for generating stream data. When we try to set up a chat history for the model, the model stopped working.\n",
        "\n",
        "To overcome this, we can create memory buffer for conversation chat, which will require extra memory and computation (https://python.langchain.com/docs/modules/memory/), or we can use a different model."
      ],
      "metadata": {
        "id": "JetfWTYJhZh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Referenced from https://learn.deeplearning.ai/pair-programming-llm/lesson/1/introduction\n",
        "* Referenced from https://developers.generativeai.google/guide"
      ],
      "metadata": {
        "id": "0yT6XvI5CMjv"
      }
    }
  ]
}